{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    embed_size = 300 # Size of the word embeddings\n",
    "    hidden_layers = 1\n",
    "    hidden_size = 64\n",
    "    output_size = 2 # no of output labels\n",
    "    max_epochs = 15\n",
    "    hidden_size_linear = 64 \n",
    "    lr = 0.5 # learning rate\n",
    "    batch_size = 128\n",
    "    seq_len = None # Sequence length for RNN\n",
    "    dropout_keep = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class Yelp_Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_iterator = None\n",
    "        self.test_iterator = None\n",
    "        self.val_iterator = None\n",
    "        self.vocab = []\n",
    "        self.word_embeddings = {}\n",
    "    \n",
    "    def load_data(self, w2v_file, train_df, test_df):\n",
    "        tokenizer = lambda sent: [x for x in word_tokenize(sent) if x != \" \"]\n",
    "        \n",
    "        # Creating Field for data\n",
    "        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "        datafields = [(\"text\",TEXT),(\"label\",LABEL)]\n",
    "        \n",
    "        # Load data from pd.DataFrame into torchtext.data.Dataset\n",
    "        \n",
    "        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
    "        train_data = data.Dataset(train_examples, datafields)\n",
    "        \n",
    "        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n",
    "        test_data = data.Dataset(test_examples, datafields)\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "        \n",
    "        TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n",
    "        self.word_embeddings = TEXT.vocab.vectors\n",
    "        self.vocab = TEXT.vocab\n",
    "        \n",
    "        self.train_iterator = data.BucketIterator(\n",
    "            (train_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "        \n",
    "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (val_data, test_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=False)\n",
    "        \n",
    "        \n",
    "\n",
    "def get_accuracy(model, iterator):\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "        y_pred = model(x)\n",
    "        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_y.extend(batch.label.numpy())\n",
    "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class RCNN(nn.Module):\n",
    "    def __init__(self, config, vocab_size, word_embeddings):\n",
    "        super(RCNN, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
    "        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "        \n",
    "        # Bi-directional LSTM for RCNN\n",
    "        self.lstm = nn.LSTM(input_size = self.config.embed_size,\n",
    "                            hidden_size = self.config.hidden_size,\n",
    "                            num_layers = self.config.hidden_layers,\n",
    "                            dropout = self.config.dropout_keep,\n",
    "                            bidirectional = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.config.dropout_keep)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.W = nn.Linear(\n",
    "            self.config.embed_size + 2*self.config.hidden_size,\n",
    "            self.config.hidden_size_linear\n",
    "        )\n",
    "        \n",
    "        # Tanh non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Fully-Connected Layer\n",
    "        self.fc = nn.Linear(\n",
    "            self.config.hidden_size_linear,\n",
    "            self.config.output_size\n",
    "        )\n",
    "        \n",
    "        # Softmax\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded_sent = self.embeddings(x)\n",
    "        lstm_out, (h_n,c_n) = self.lstm(embedded_sent)\n",
    "        input_features = torch.cat([lstm_out,embedded_sent], 2).permute(1,0,2)\n",
    "        linear_output = self.tanh(\n",
    "            self.W(input_features)\n",
    "        )\n",
    "        linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n",
    "        max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n",
    "        max_out_features = self.dropout(max_out_features)\n",
    "        final_out = self.fc(max_out_features)\n",
    "        return self.softmax(final_out)\n",
    "    \n",
    "    def add_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def add_loss_op(self, loss_op):\n",
    "        self.loss_op = loss_op\n",
    "    \n",
    "    def reduce_lr(self):\n",
    "        print(\"Reducing LR\")\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] / 2\n",
    "                \n",
    "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
    "        train_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        # Reduce learning rate as number of epochs increase\n",
    "        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n",
    "            self.reduce_lr()\n",
    "            \n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            self.optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                x = batch.text.cuda()\n",
    "                y = (batch.label - 1).type(torch.cuda.LongTensor)\n",
    "            else:\n",
    "                x = batch.text\n",
    "                y = (batch.label - 1).type(torch.LongTensor)\n",
    "            y_pred = self.__call__(x)\n",
    "            loss = self.loss_op(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                print(\"Iteration: {}\".format(i+1))\n",
    "                avg_train_loss = np.mean(losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
    "                losses = []\n",
    "                \n",
    "                # Evalute Accuracy on validation set\n",
    "                val_accuracy = get_accuracy(self, val_iterator)\n",
    "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
    "                self.train()\n",
    "                \n",
    "        return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sampr\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sampr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1\n",
      "\tAverage training loss: -0.24101\n",
      "\tVal Accuracy: 0.2552\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.49943\n",
      "\tVal Accuracy: 0.7842\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.76596\n",
      "\tVal Accuracy: 0.8299\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.81653\n",
      "\tVal Accuracy: 0.8370\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.82595\n",
      "\tVal Accuracy: 0.8459\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.83443\n",
      "\tVal Accuracy: 0.8490\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.84113\n",
      "\tVal Accuracy: 0.8535\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.83926\n",
      "\tVal Accuracy: 0.8561\n",
      "Epoch: 1\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.83464\n",
      "\tVal Accuracy: 0.8598\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.85123\n",
      "\tVal Accuracy: 0.8597\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.85632\n",
      "\tVal Accuracy: 0.8630\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.85187\n",
      "\tVal Accuracy: 0.8614\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.85711\n",
      "\tVal Accuracy: 0.8630\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.85542\n",
      "\tVal Accuracy: 0.8642\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.86220\n",
      "\tVal Accuracy: 0.8692\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.86390\n",
      "\tVal Accuracy: 0.8672\n",
      "Epoch: 2\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.86017\n",
      "\tVal Accuracy: 0.8687\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.86546\n",
      "\tVal Accuracy: 0.8707\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.86570\n",
      "\tVal Accuracy: 0.8707\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.87039\n",
      "\tVal Accuracy: 0.8748\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.87160\n",
      "\tVal Accuracy: 0.8705\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.87761\n",
      "\tVal Accuracy: 0.8785\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.86924\n",
      "\tVal Accuracy: 0.8805\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.87442\n",
      "\tVal Accuracy: 0.8737\n",
      "Epoch: 3\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.88857\n",
      "\tVal Accuracy: 0.8782\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.87577\n",
      "\tVal Accuracy: 0.8766\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.87500\n",
      "\tVal Accuracy: 0.8837\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.87367\n",
      "\tVal Accuracy: 0.8807\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.88030\n",
      "\tVal Accuracy: 0.8829\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.87882\n",
      "\tVal Accuracy: 0.8810\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.87668\n",
      "\tVal Accuracy: 0.8868\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88644\n",
      "\tVal Accuracy: 0.8888\n",
      "Epoch: 4\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.90763\n",
      "\tVal Accuracy: 0.8845\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.87550\n",
      "\tVal Accuracy: 0.8736\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.88376\n",
      "\tVal Accuracy: 0.8883\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.88709\n",
      "\tVal Accuracy: 0.8897\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.88797\n",
      "\tVal Accuracy: 0.8890\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89086\n",
      "\tVal Accuracy: 0.8823\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88052\n",
      "\tVal Accuracy: 0.8865\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88482\n",
      "\tVal Accuracy: 0.8879\n",
      "Epoch: 5\n",
      "Reducing LR\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.91650\n",
      "\tVal Accuracy: 0.8895\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.88767\n",
      "\tVal Accuracy: 0.8924\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.88980\n",
      "\tVal Accuracy: 0.8915\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.89497\n",
      "\tVal Accuracy: 0.8893\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.89105\n",
      "\tVal Accuracy: 0.8902\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89462\n",
      "\tVal Accuracy: 0.8934\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.89002\n",
      "\tVal Accuracy: 0.8895\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89845\n",
      "\tVal Accuracy: 0.8988\n",
      "Epoch: 6\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.89869\n",
      "\tVal Accuracy: 0.8908\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.89259\n",
      "\tVal Accuracy: 0.8988\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.89425\n",
      "\tVal Accuracy: 0.8966\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.89337\n",
      "\tVal Accuracy: 0.8901\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.89722\n",
      "\tVal Accuracy: 0.8961\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89764\n",
      "\tVal Accuracy: 0.8895\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.89760\n",
      "\tVal Accuracy: 0.8981\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89628\n",
      "\tVal Accuracy: 0.8996\n",
      "Epoch: 7\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.88061\n",
      "\tVal Accuracy: 0.8954\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.89586\n",
      "\tVal Accuracy: 0.8940\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.89255\n",
      "\tVal Accuracy: 0.8978\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.89972\n",
      "\tVal Accuracy: 0.8982\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.89688\n",
      "\tVal Accuracy: 0.9004\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89973\n",
      "\tVal Accuracy: 0.9012\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.89640\n",
      "\tVal Accuracy: 0.9007\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89960\n",
      "\tVal Accuracy: 0.8962\n",
      "Epoch: 8\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.87265\n",
      "\tVal Accuracy: 0.9022\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.89954\n",
      "\tVal Accuracy: 0.9015\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.89914\n",
      "\tVal Accuracy: 0.8995\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90139\n",
      "\tVal Accuracy: 0.8999\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90585\n",
      "\tVal Accuracy: 0.9001\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89651\n",
      "\tVal Accuracy: 0.8980\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90108\n",
      "\tVal Accuracy: 0.9017\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89851\n",
      "\tVal Accuracy: 0.9008\n",
      "Epoch: 9\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.93523\n",
      "\tVal Accuracy: 0.9010\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90422\n",
      "\tVal Accuracy: 0.9010\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90391\n",
      "\tVal Accuracy: 0.8989\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90292\n",
      "\tVal Accuracy: 0.9010\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90434\n",
      "\tVal Accuracy: 0.9028\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89975\n",
      "\tVal Accuracy: 0.9010\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90348\n",
      "\tVal Accuracy: 0.8975\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89585\n",
      "\tVal Accuracy: 0.8982\n",
      "Epoch: 10\n",
      "Reducing LR\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.90961\n",
      "\tVal Accuracy: 0.8989\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90577\n",
      "\tVal Accuracy: 0.8998\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90314\n",
      "\tVal Accuracy: 0.9035\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90655\n",
      "\tVal Accuracy: 0.8988\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90633\n",
      "\tVal Accuracy: 0.9029\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90355\n",
      "\tVal Accuracy: 0.8984\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90801\n",
      "\tVal Accuracy: 0.9048\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90630\n",
      "\tVal Accuracy: 0.9012\n",
      "Epoch: 11\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.88156\n",
      "\tVal Accuracy: 0.9026\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90679\n",
      "\tVal Accuracy: 0.9037\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90770\n",
      "\tVal Accuracy: 0.8992\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90466\n",
      "\tVal Accuracy: 0.9053\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90709\n",
      "\tVal Accuracy: 0.9050\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90944\n",
      "\tVal Accuracy: 0.9034\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90765\n",
      "\tVal Accuracy: 0.9032\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90565\n",
      "\tVal Accuracy: 0.9018\n",
      "Epoch: 12\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.92182\n",
      "\tVal Accuracy: 0.9041\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90668\n",
      "\tVal Accuracy: 0.9040\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90324\n",
      "\tVal Accuracy: 0.9003\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90801\n",
      "\tVal Accuracy: 0.9045\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90727\n",
      "\tVal Accuracy: 0.9054\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90586\n",
      "\tVal Accuracy: 0.9025\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90664\n",
      "\tVal Accuracy: 0.9040\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90710\n",
      "\tVal Accuracy: 0.9038\n",
      "Epoch: 13\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.81755\n",
      "\tVal Accuracy: 0.9045\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90847\n",
      "\tVal Accuracy: 0.9034\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90977\n",
      "\tVal Accuracy: 0.9033\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90909\n",
      "\tVal Accuracy: 0.9040\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90731\n",
      "\tVal Accuracy: 0.9029\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90624\n",
      "\tVal Accuracy: 0.9062\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.91039\n",
      "\tVal Accuracy: 0.9040\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90472\n",
      "\tVal Accuracy: 0.9038\n",
      "Epoch: 14\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.93614\n",
      "\tVal Accuracy: 0.9047\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90433\n",
      "\tVal Accuracy: 0.9037\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.91165\n",
      "\tVal Accuracy: 0.9027\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90377\n",
      "\tVal Accuracy: 0.9073\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90934\n",
      "\tVal Accuracy: 0.9048\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.91136\n",
      "\tVal Accuracy: 0.9075\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.91099\n",
      "\tVal Accuracy: 0.9062\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90864\n",
      "\tVal Accuracy: 0.9046\n",
      "Final Training Accuracy: 0.9113\n",
      "Final Validation Accuracy: 0.9052\n",
      "Final Test Accuracy: 0.9033\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "config = Config()\n",
    "train_file = pd.read_csv('../data/yelp_train.csv')\n",
    "test_file = pd.read_csv('../data/yelp_test.csv')\n",
    "# for testing \n",
    "#train_file = train_file.iloc[:1000,:]\n",
    "#test_file = test_file.iloc[:1000,:]\n",
    "\n",
    "\n",
    "# Glove embeddings\n",
    "w2v_file = '../data/glove.840B.300d.txt'\n",
    "\n",
    "dataset = Yelp_Dataset(config)\n",
    "dataset.load_data(w2v_file, train_file, test_file)\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# call the model\n",
    "model = RCNN(config, len(dataset.vocab), dataset.word_embeddings)\n",
    "# if gpu\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# train \n",
    "model.train()\n",
    "optimizer = optim.SGD(model.parameters(), lr=config.lr)\n",
    "NLLLoss = nn.NLLLoss()\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(NLLLoss)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# epochs 15\n",
    "for i in range(config.max_epochs):\n",
    "    print (\"Epoch: {}\".format(i))\n",
    "    train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "train_acc = get_accuracy(model, dataset.train_iterator)\n",
    "val_acc = get_accuracy(model, dataset.val_iterator)\n",
    "test_acc = get_accuracy(model, dataset.test_iterator)\n",
    "\n",
    "print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
