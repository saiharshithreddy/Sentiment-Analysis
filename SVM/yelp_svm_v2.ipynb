{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import langid\n",
    "from multiprocessing import  Pool\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(x):\n",
    "    '''\n",
    "    1. Takes the input and detects the language and assigns it to the lang\n",
    "    2. Returns the first element in lang\n",
    "    '''\n",
    "    lang = langid.classify(x)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review():\n",
    "    '''\n",
    "    1. Importing the csv file into a dataframe\n",
    "    2. Dropping columns other than business_id, stars and text\n",
    "    3. Applying the language detection on the text reviews\n",
    "    4. Filter the dataframe based on language of text \n",
    "    5. Return the dataframe\n",
    "    '''\n",
    "    \n",
    "    reviews = pd.read_csv('review.csv')\n",
    "    \n",
    "    reviews = reviews.loc[:, [\"business_id\", \"stars\", \"text\"]]\n",
    "\n",
    "    reviews['lang'] = reviews['text'].apply(lambda row: detect_lang(row))\n",
    "    \n",
    "    review = reviews.loc[reviews['lang'] == 'en']\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    '''\n",
    "    1. Takes the input and checks if its positive or negative or neutral and assigns it to the label\n",
    "    2. Returns the label\n",
    "    '''\n",
    "    label = 'positive' if x >= 0.0 else 'negative'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(x):\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Assigns the varaibles to dataframe\n",
    "    2. Calculates Polarity of each review and returns it to a new column in yelp dataframe\n",
    "    3. Calculates Sentiment_lablel of each review and returns it to a new column in yelp dataframe\n",
    "    4. Returns the dataframe\n",
    "    '''\n",
    "        \n",
    "    yelp = x\n",
    "    \n",
    "    yelp['sentiment'] = yelp['text'].apply(lambda review: TextBlob(review).polarity)\n",
    "    \n",
    "    yelp['sentiment_label'] = yelp['sentiment'].apply(lambda row: check(row))\n",
    "    \n",
    "    yelp = yelp.dropna()\n",
    "    \n",
    "    yelp = yelp.loc[:, [ \"text\",\"sentiment_label\"]]\n",
    "    return yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(df, func, n_cores=2):\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. The dataframe is split into n_cores and the function is performed on each sub dataframe.\n",
    "    2. The sub data frames are merged together and returned.\n",
    "    '''\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Convert text to lower case\n",
    "    2. Remove all punctuation\n",
    "    3. Remove all stopwords except for not\n",
    "    4. Return the cleaned text as a bag of words\n",
    "    '''\n",
    "    import string\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove punctuations\n",
    "    text = [char for char in text if char not in string.punctuation]\n",
    "    text = ''.join(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.remove(\"not\")\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word in stops]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    y_df = add_label(data)\n",
    "    #applies the function on each value of the pandas series\n",
    "    y_df['text'] = y_df['text'].apply(text_process)\n",
    "    return y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_features_tfid(data):\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Create a vectorizer variable and assigns the scikit learn Tfidvectorizer to it\n",
    "    2. Converts the input variable into vectors using the vectorizer\n",
    "    3. Converts the vectors to an array and assigns the array to a variable\n",
    "    4. Returns the vector array variable\n",
    "    '''\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    # Create feature vectors\n",
    "    vectorizer = TfidfVectorizer(analyzer = 'word',\n",
    "                                 min_df = 5,\n",
    "                                 max_df = 0.8,\n",
    "                                 sublinear_tf = True,\n",
    "                                 use_idf = True)\n",
    "\n",
    "    features = vectorizer.fit_transform(data)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_features_count(data):\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Create a vectorizer variable and assigns the scikit learn countvectorizer to it\n",
    "    2. Converts the input variable into vectors using the vectorizer\n",
    "    3. Converts the vectors to an array and assigns the array to a variable\n",
    "    4. Returns the vector array variable\n",
    "    '''\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    # Create feature vectors\n",
    "    vectorizer = CountVectorizer(analyzer = 'word')\n",
    "\n",
    "    features = vectorizer.fit_transform(data)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_then_build_model(features_A,B):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Splits the variables into training and test sets in ratio of 80,20\n",
    "    2. Fits the training and test data into a logistic regression model and starts trsining\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(features_A, B, train_size=0.80, random_state=1234)    \n",
    "         \n",
    "    from sklearn import svm\n",
    "    # Perform classification with SVM, kernel=linear\n",
    "    classifier_linear = svm.SVC(kernel='linear')\n",
    "    classifier_linear.fit(X=X_train, y=y_train)\n",
    "    pred = classifier_linear.predict(X_test)\n",
    "    \n",
    "    return pred,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(p,q):\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Calculate the accuracy, classification report of the model and prints it\n",
    "    '''\n",
    "    y_test = p\n",
    "    pred = q\n",
    "    \n",
    "    print(\"Accuracy :: {}\".format(round(accuracy_score(y_test,pred)*100,2)))\n",
    "    print(\"Classification Report ::\")\n",
    "    print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('cleaned_yelp_reviews.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :: 88.4\n",
      "Classification Report ::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74      1300\n",
      "           1       0.93      0.92      0.93      4700\n",
      "\n",
      "    accuracy                           0.88      6000\n",
      "   macro avg       0.83      0.84      0.83      6000\n",
      "weighted avg       0.89      0.88      0.89      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_A= transform_to_features_count(df[\"sentence\"]) \n",
    "count_y_test,count_pred = train_then_build_model(features_A,df['sentiment'])\n",
    "test_metrics(count_y_test,count_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :: 90.9\n",
      "Classification Report ::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79      1192\n",
      "           1       0.96      0.92      0.94      4808\n",
      "\n",
      "    accuracy                           0.91      6000\n",
      "   macro avg       0.85      0.89      0.86      6000\n",
      "weighted avg       0.92      0.91      0.91      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_X= transform_to_features_tfid(df[\"sentence\"])\n",
    "tfid_y_test,tfid_pred = train_then_build_model(features_X,df['sentiment'])\n",
    "test_metrics(tfid_y_test,tfid_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
