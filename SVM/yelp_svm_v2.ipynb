{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import langid\n",
    "from multiprocessing import  Pool\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(x):\n",
    "    '''\n",
    "    1. Takes the input and detects the language and assigns it to the lang\n",
    "    2. Returns the first element in lang\n",
    "    '''\n",
    "    lang = langid.classify(x)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review():\n",
    "    '''\n",
    "    1. Importing the csv file into a dataframe\n",
    "    2. Dropping columns other than business_id, stars and text\n",
    "    3. Applying the language detection on the text reviews\n",
    "    4. Filter the dataframe based on language of text \n",
    "    5. Return the dataframe\n",
    "    '''\n",
    "    \n",
    "    reviews = pd.read_csv('review.csv')\n",
    "    \n",
    "    reviews = reviews.loc[:, [\"business_id\", \"stars\", \"text\"]]\n",
    "\n",
    "    reviews['lang'] = reviews['text'].apply(lambda row: detect_lang(row))\n",
    "    \n",
    "    review = reviews.loc[reviews['lang'] == 'en']\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    '''\n",
    "    1. Takes the input and checks if its positive or negative or neutral and assigns it to the label\n",
    "    2. Returns the label\n",
    "    '''\n",
    "    label = 'positive' if x >= 0.0 else 'negative'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(x):\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Assigns the varaibles to dataframe\n",
    "    2. Calculates Polarity of each review and returns it to a new column in yelp dataframe\n",
    "    3. Calculates Sentiment_lablel of each review and returns it to a new column in yelp dataframe\n",
    "    4. Assigns the text, stars and sentiment_label to 2 variables and returns the variables\n",
    "    '''\n",
    "        \n",
    "    yelp = x\n",
    "    \n",
    "    yelp['sentiment'] = yelp['text'].apply(lambda review: TextBlob(review).polarity)\n",
    "    \n",
    "    yelp['sentiment_label'] = yelp['sentiment'].apply(lambda row: check(row))\n",
    "    \n",
    "    yelp = yelp.dropna()\n",
    "    \n",
    "    yelp = yelp.loc[:, [ \"text\",\"sentiment_label\"]]\n",
    "    return yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(df, func, n_cores=2):\n",
    "    '''\n",
    "    The dataframe is split into n_cores and availability_filter is performed on each sub dataframe\n",
    "    n_cores: No of processors (96 in this env)\n",
    "    n_cores is taken as 10 as it takes less time to split the given data.\n",
    "    '''\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Return the cleaned text as a list of words\n",
    "    '''\n",
    "    import string\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove punctuations\n",
    "    text = [char for char in text if char not in string.punctuation]\n",
    "    text = ''.join(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.remove(\"not\")\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word in stops]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    y_df = add_label(data)\n",
    "    #applies the function on each value of the pandas series\n",
    "    y_df['text'] = y_df['text'].apply(text_process)\n",
    "    return y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_features(data):\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Create a vectorizer variable and assigns the scikit learn countvectorizer to it\n",
    "    2. Converts the input variable into vectors using the vectorizer\n",
    "    3. Converts the vectors to an array and assigns the array to a variable\n",
    "    4. Returns the vector array variable\n",
    "    '''\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    # Create feature vectors\n",
    "    vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                                 max_df = 0.8,\n",
    "                                 sublinear_tf = True,\n",
    "                                 use_idf = True)\n",
    "\n",
    "    features = vectorizer.fit_transform(data)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_then_build_model(features_A,B):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Splits the variables into training and test sets in ratio of 80,20\n",
    "    2. Fits the training and test data into a logistic regression model and starts trsining\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(features_A, B, train_size=0.80, random_state=1234)    \n",
    "         \n",
    "    from sklearn import svm\n",
    "    # Perform classification with SVM, kernel=linear\n",
    "    classifier_linear = svm.SVC(kernel='linear')\n",
    "    classifier_linear.fit(X=X_train, y=y_train)\n",
    "    pred = classifier_linear.predict(X_test)\n",
    "    \n",
    "    return pred,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(p,q):\n",
    "    '''\n",
    "    Takes in variables, then performs the following:\n",
    "    1. Calculate the accuracy, classification report of the model and prints it\n",
    "    2. Prints the first 5 reviews thst were predicted correctly\n",
    "    3. Prints the first 5 reviews that were not predicted correctly\n",
    "    '''\n",
    "    y_test = p\n",
    "    pred = q\n",
    "    \n",
    "    print(\"Accuracy :: {}\".format(round(accuracy_score(y_test,pred)*100,2)))\n",
    "    print(\"Classification Report ::\")\n",
    "    print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('cleaned_yelp_reviews.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_A= transform_to_features(df[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test,pred = train_then_build_model(features_A,df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :: 90.98\n",
      "Classification Report ::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79      1197\n",
      "           1       0.96      0.92      0.94      4803\n",
      "\n",
      "    accuracy                           0.91      6000\n",
      "   macro avg       0.85      0.89      0.87      6000\n",
      "weighted avg       0.92      0.91      0.91      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_metrics(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
